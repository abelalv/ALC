[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Algebra lineal Computacional",
    "section": "",
    "text": "1 Prefacio\nEstas son las notas de la clase de Algebra lineal computacional, en ningún momento pretenden ser un libro de texto, ni mucho menos un sustituto de las clases. Son un apoyo para los estudiantes que deseen repasar los temas vistos en clase."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2  Introducción",
    "section": "",
    "text": "En álgebra lineal computacional es una herramienta que esta ne auge en la actualidad, ya que permite resolver diversos problemas y va de la mano con la eficiencia computacional. En este curso haremos una pequeña mirada al álgebra lineal computacional y resolveremos diferentes aplicaciones usando el lenguaje de programación Python. la primera parte de este curso se usará el libro Strang (2022) como guía. En el segunda parte del curso usaremos Sauer (2018) como guía.\n\n\n\n\nSauer, T. 2018. Numerical Analysis. Pearson Education. https://books.google.com.co/books?id=gAVTDwAAQBAJ.\n\n\nStrang, Gilbert. 2022. Introduction to Linear Algebra. SIAM."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Sauer, T. 2018. Numerical Analysis. Pearson Education. https://books.google.com.co/books?id=gAVTDwAAQBAJ.\n\n\nStrang, Gilbert. 2022. Introduction to Linear Algebra. SIAM."
  },
  {
    "objectID": "summary.html#ejemplos",
    "href": "summary.html#ejemplos",
    "title": "3  Espacios vectoriales",
    "section": "3.1 Ejemplos",
    "text": "3.1 Ejemplos\n\n\\(\\mathcal{R}̣\\)\n\\(\\mathcal{C}\\)\n\\(\\mathcal{R}^n\\)\n\\(\\mathcal{C}^n\\)\nEl conjunto de todos los vectores ortogonales a un vector dado\nEl conjunto de todas las funciones definidas en \\(\\mathcal{R}\\)\nEl conjunto de todas las funciones continuas en un intervalo dado\nEl conjunto de todas las matrices del mismo tamaño\nEl conjunto de todas las funciones diferenciables en un intervalo dado\nEl conjunto de todas las funciones que satisfacen la ecuación diferencial \\[y''+ay'+by.\\]\n\n\n3.1.1 Subespacios de un espacio vectorial\nSea \\(V\\) un espacio vectorial y \\(S\\) un subconjunto de \\(V\\), \\(S\\) es un subespacio de \\(V\\), si en si mismo es un espacio vectorial.\n\n\n\n\n\n\nTeorema\n\n\n\nSea \\(S\\) un subconjunto no vació de un espacio vectorial \\(V\\). Si los elementos de \\(S\\) satisfacen los axiomas de clausura entonces \\(S\\) es un subespacio de \\(V\\)\n\n\n\n\n\n\n\n\nDefinición\n\n\n\nSea \\(S\\) un subconjunto no vació de un espacio vectorial \\(V\\), un elemento \\(x\\) de \\(V\\) de la forma: \\[x=\\sum_{i=1}^{k}c_ix_i,\\] en donde \\(x_1,x_2,...,x_k\\) son elementos de \\(S\\) y \\(c_i\\) son escalares, se denomina la combinación lineal de elementos de \\(S\\).\n\n\n\n\n\n\n\n\nTeorema\n\n\n\nEl conjunto \\(span(S)=\\{\\sum_{i=1}^{k}c_ix_i,|c_i\\in F\\}\\) de todos los elementos generados en combinación lineal de \\(S\\) es un subespacio vectorial de \\(V\\)\n\n\n\n\n\n\n\n\nNote\n\n\n\nSi \\(S\\) es un subespacio de \\(V\\) entonces \\(S=span(S)\\). Además, el generado \\(gen(S)=span(S)\\)\n\n\nSupongamos que queremos calcular el \\(span(S)\\) donde \\(S\\{x_1,x_2,...,x_n\\}\\), y \\(x_i\\in\\mathbb{R^n}\\), para esto debemos calcular todas las combinaciones lineales de los elementos de \\(S\\) de la siguinete forma\n\\[span(S)=\\{\\sum_{i=1}^{n}c_ix_i,|c_i\\in\\mathbb{R}\\}\\]\nOtra forma de calcular esta combinación es crear una matriz \\(M\\) con todos los vectores de \\(S\\) y multiplicarla por un vector columna de los coeficientes \\(c_i\\), de esta forma obtenemos un vector columna con todos los elementos de la combinación lineal, esto se puedo expresar como la multiplicación de una matriz por un vector columna.\n\\[span(S)=M\\in\\mathbb{R}c\\]\n\n\n\n\n\n\nDefinición\n\n\n\nSea \\(A\\) una matriz de \\(m\\times n\\), definimos como \\(Im(A)=span\\{y\\in\\mathbb{R^m}|Ax=y\\}\\), note si tomamos \\(x=e_i\\) tenemos donde \\(Ae_i=A_i\\) son las columnas de \\(A\\). Además definimos \\(Ker(A)=\\{x\\in\\mathbb{R^n}|Ax=0\\}\\)."
  },
  {
    "objectID": "subespacios.html",
    "href": "subespacios.html",
    "title": "6  Subespacios principales de una matriz",
    "section": "",
    "text": "Una matriz \\(A_{m\\times n}\\) crea cuatro subespacios importantes, como los podemos mostrar en la siguiente figura. (Aquí se puede encontrar màs inforacion del tema)\n\n\n\n4 espacios fundamentales\n\n\nLlamaremos \\(C(A)\\) como el espacio columna de la matriz \\(A\\) y \\(N(A)\\) como el espacio nulo de la matriz de \\(A\\). De aquì podemos observar las siguientes propiedades\n\\[C(A^T)\\perp N(A).\\] Para mostrar esta proiedad debemos mostrar que si \\(x_1\\in C(A^T)\\) y \\(x_2\\in N(A)\\), entonces\n\\[\\langle x_1,x_2\\rangle=x_2^Tx_1=0.\\]\nSabemos que existe un \\(y\\in \\mathbb{R}^m\\), note que lo podemos descomponer en \\(y=y_1+y_2\\), donde \\(y_1\\in C(A)\\) y \\(y_2\\in N(A^T)\\) luego \\[A^Ty=A^T(y_1+y_2)=A^ty_1=x_1.\\]\nAhora tenemos que\n\\[\\langle x_2,x_1\\rangle=(A^Ty)^Tx_2=y^TAx_2=0,\\] por tanto son ortogonales. Con un argumento similar podemos mostrar que \\(C(A)\\perp N(A^T)\\).\nAdemas tenemos la propiedad que \\[dim(C(A))=dim(N(A^T))=r,\\] de esta forma tenemos que \\(dim(N(A))=n-r\\) y \\(dim(N(A^T))=m-r\\).\n\n\n\n\n\n\nDefinición\n\n\n\nSea \\(A\\) una matriz de \\(m\\times n\\), definimos como el \\(rank(A)\\) como el numero máximo de filas o columnas lienalmente independientes de \\(A\\). Note que por la propiedad anterior tenemos que \\(rank(A)=dim(C(A))=dim(N(A^T))\\).\n\n\nAlgunas propiedades importantes del rank son las siguientes\n\nSi \\(A\\) es una matriz cuadrada y \\(rank(A)=n\\) entonces todos autovalores de \\(A\\) son distintos de cero.\nSi \\(A\\) es una matriz cuadrada entonces \\(rank(A)=n\\) si y solo si \\(A\\) es invertible.\n\nPara demostrar la primera vamos a suponer que es falso por tanto 0 es un autovalor luego \\(Ax=0\\), lo que implica que \\(N(A)\\neq\\emptyset\\), esto contradice el hecho que \\(rank(A)=dim(C(A^T))=dim(C(A))=n\\). Ahora, si \\(A\\) tiene todos los autovalores diferentes de cero entonces \\(A\\) es invertible, por lo tanto \\(rank(A)=n\\).\n\nSi \\(A\\) una matriz de \\(m\\times n\\), con \\(m>n\\) entonces \\(A^TA\\) es invertible.\n\nSea \\(x\\in \\mathbb{R}^n\\) entonces \\(Ax=y\\neq 0\\) puesto que \\(rank(A)=n,\\) entonces \\(y\\notin N(A^T)\\). De esta forma existe un \\(x\\neq 0\\) tal que \\(x=A^Ty\\) y que \\(x\\in C(A^T)\\) y \\(x\\neq 0\\). Para completar la demostración debemos debemos mostrar que \\(dim(Im(A^TA))=n\\).\nPara esto vamos a usar la propiedad que \\(rank(A)=n\\) y \\(rank(A^T)=n\\). Entonces tenemos que \\(rank(A^TA)=n\\) y por lo tanto \\(A^TA\\) es invertible."
  },
  {
    "objectID": "espacios_vectoriales.html#ejemplos",
    "href": "espacios_vectoriales.html#ejemplos",
    "title": "3  Espacios vectoriales",
    "section": "3.1 Ejemplos",
    "text": "3.1 Ejemplos\n\n\\(\\mathcal{R}̣\\)\n\\(\\mathcal{C}\\)\n\\(\\mathcal{R}^n\\)\n\\(\\mathcal{C}^n\\)\nEl conjunto de todos los vectores ortogonales a un vector dado\nEl conjunto de todas las funciones definidas en \\(\\mathcal{R}\\)\nEl conjunto de todas las funciones continuas en un intervalo dado\nEl conjunto de todas las matrices del mismo tamaño\nEl conjunto de todas las funciones diferenciables en un intervalo dado\nEl conjunto de todas las funciones que satisfacen la ecuación diferencial \\[y''+ay'+by.\\]\n\n\n3.1.1 Subespacios de un espacio vectorial\nSea \\(V\\) un espacio vectorial y \\(S\\) un subconjunto de \\(V\\), \\(S\\) es un subespacio de \\(V\\), si en si mismo es un espacio vectorial.\n\n\n\n\n\n\nTeorema\n\n\n\nSea \\(S\\) un subconjunto no vació de un espacio vectorial \\(V\\). Si los elementos de \\(S\\) satisfacen los axiomas de clausura entonces \\(S\\) es un subespacio de \\(V\\)\n\n\n\n\n\n\n\n\nDefinición\n\n\n\nSea \\(S\\) un subconjunto no vació de un espacio vectorial \\(V\\), un elemento \\(x\\) de \\(V\\) de la forma: \\[x=\\sum_{i=1}^{k}c_ix_i,\\] en donde \\(x_1,x_2,...,x_k\\) son elementos de \\(S\\) y \\(c_i\\) son escalares, se denomina la combinación lineal de elementos de \\(S\\).\n\n\n\n\n\n\n\n\nTeorema\n\n\n\nEl conjunto \\(span(S)=\\{\\sum_{i=1}^{k}c_ix_i,|c_i\\in F\\}\\) de todos los elementos generados en combinación lineal de \\(S\\) es un subespacio vectorial de \\(V\\)\n\n\n\n\n\n\n\n\nNote\n\n\n\nSi \\(S\\) es un subespacio de \\(V\\) entonces \\(S=span(S)\\). Además, el generado \\(gen(S)=span(S)\\)\n\n\nSupongamos que queremos calcular el \\(span(S)\\) donde \\(S\\{x_1,x_2,...,x_n\\}\\), y \\(x_i\\in\\mathbb{R^n}\\), para esto debemos calcular todas las combinaciones lineales de los elementos de \\(S\\) de la siguiente forma\n\\[span(S)=\\{\\sum_{i=1}^{n}c_ix_i,|c_i\\in\\mathbb{R}\\}\\]\nOtra forma de calcular esta combinación es crear una matriz \\(M\\) con todos los vectores de \\(S\\) y multiplicarla por un vector columna de los coeficientes \\(c_i\\), de esta forma obtenemos un vector columna con todos los elementos de la combinación lineal, esto se puedo expresar como la multiplicación de una matriz por un vector columna.\n\\[span(S)=M\\in\\mathbb{R}c\\]\n\n\n\n\n\n\nDefinición\n\n\n\nSea \\(A\\) una matriz de \\(m\\times n\\), definimos como el espacio columna de \\(A\\)\n\\[C(A)=span\\{y\\in\\mathbb{R^m}|Ax=y\\},\\]\ny este se denota por \\(C(A)\\).\nNote si tomamos \\(x=e_i\\) tenemos donde \\(Ae_i=A_{\\cdot i}\\) es \\(i-esima\\) columna de \\(A\\). Además definimos como el espacio nulo de \\(A\\)\n\\[N(A)=span\\{x\\in\\mathbb{R^n}|Ax=0\\}.\\]\n\n\n\n\n\n\n\n\nDefinición\n\n\n\nSe \\(S=\\{x_1,x_2,...,x_n\\}\\) un conjunto de vectores linealmente independientes de \\(V\\), entonces se dice que \\(S\\) es una base de \\(V\\) si genera a \\(V\\).\n\n\nSi el conjunto \\(S\\) es finito, entonces se dice que \\(V\\) es de dimensión finita y se denota por \\(dim(V)=n\\). Existen espacios con dimensión infinita, pero no los estudiaremos en este curso.\n\n\n\n\n\n\nDefinición\n\n\n\nEl rank de una matriz \\(A\\) es la dimensión del espacio columna de \\(A\\) y se denota por \\(rank(A)\\).\n\n\n\n\n\n\n\n\nDefinición\n\n\n\nSea \\(A\\) una matriz de \\(m\\times n\\), con \\(m>n\\) \\(rank(A)=n\\) diremos que \\(A\\) es una matriz de rango completo.\n\n\n\n\n\n\n\n\nEjercicio\n\n\n\n\n\n\nMuestre que si una matriz \\(A\\) todas sus columnas son linealmente independientes entonces el espacio nulo de \\(A\\) es el vector \\(\\{0\\}\\).\nSi \\(A\\) es una matriz de \\(m\\times n\\) y \\(B\\) es una matriz de \\(n\\times p\\) muestre que el espacio nulo de \\(AB\\) es un subespacio del espacio nulo de \\(B\\).\nSi \\(A\\) es una matriz de \\(n\\times n\\) y la dimensión de su espacio columna es \\(n\\) entonces la inversa de \\(A\\) existe."
  },
  {
    "objectID": "espacios_vectoriales.html",
    "href": "espacios_vectoriales.html",
    "title": "3  Espacios vectoriales",
    "section": "",
    "text": "4 Solución de sistema de ecuaciones lineales"
  },
  {
    "objectID": "espacios_vectoriales.html#ejercicio",
    "href": "espacios_vectoriales.html#ejercicio",
    "title": "3  Espacios vectoriales",
    "section": "3.2 Ejercicio",
    "text": "3.2 Ejercicio\nMuestre que si una matriz \\(A\\) todas sus columnas son linealmente independientes entonces el espacio nulo de \\(A\\) es el vector \\(\\{0\\}\\). #:::"
  },
  {
    "objectID": "espacios_vectoriales.html#ejercicio-1",
    "href": "espacios_vectoriales.html#ejercicio-1",
    "title": "3  Espacios vectoriales",
    "section": "3.2 Ejercicio",
    "text": "3.2 Ejercicio\n#:::"
  },
  {
    "objectID": "producto.html",
    "href": "producto.html",
    "title": "6  Producto interno en un espacio vectorial",
    "section": "",
    "text": "7 Do something with the image\nimage.show()"
  },
  {
    "objectID": "producto.html#ejemplos",
    "href": "producto.html#ejemplos",
    "title": "6  Producto interno en un espacio vectorial",
    "section": "6.1 Ejemplos",
    "text": "6.1 Ejemplos\n\nEl producto punto en \\(\\mathbb{R}^n\\) es un producto interno.\nSea \\(A\\) una matriz simétrica definida positiva en \\(\\mathbb{R}^n\\) define un producto interno.\nSea \\(C^n\\) el espacio de todas las funciones continuas en el intervalo \\([a,b]\\). Entonces \\(f,g\\in C^n\\) se define el producto interno como \\[\\int_a^b f(x)g(x)dx.\\]\n\n\n\n\n\n\n\nTeorema\n\n\n\nEn un espacio vectorial con producto interno \\(V\\), todo producto interno satisfacen la desigualdad de Cauchy-Schwarz, es decir, para todo \\(v,w\\in V\\) se tiene que \\[|\\langle v,w \\rangle|^2\\leq \\langle v,v \\rangle\\langle w,w \\rangle.\\]\n\n\n\n\n\n\n\n\nDemostración\n\n\n\n\n\nSea \\(v,w\\in V\\) y \\(\\lambda\\in \\mathbb{R}\\), entonces, si \\(w=0\\) o \\(v=0\\) se tiene que la desigualdad se cumple trivialmente. Supongamos que \\(v\\) y \\(w\\) ambos no son cero. Sea \\(z=av+bw\\) con \\(a,b\\in \\mathbb{R}\\), entonces\n\\[0\\leq \\langle z,z \\rangle=\\langle av+bw,av+bw \\rangle=a^2\\langle v,v \\rangle+ab\\langle v,w \\rangle+ba\\langle w,v \\rangle+b^2\\langle w,w \\rangle,\\] tomando \\(a=\\langle w,w \\rangle\\) y \\(b=-\\langle v,w \\rangle,\\) se tiene que \\[0\\leq \\langle w,w \\rangle^2\\langle v,v \\rangle-2\\langle v,w \\rangle\\langle w,w \\rangle\\langle v,w \\rangle+\\langle v,w \\rangle^2\\langle w,w \\rangle=\\langle w,w \\rangle\\langle v,v \\rangle-\\langle v,w \\rangle^2,\\] de donde se sigue que \\[\\langle v,w \\rangle^2\\leq \\langle v,v \\rangle\\langle w,w \\rangle.\\]\n\n\n\n\n\n\n\n\n\nDefinición\n\n\n\nNorma inducida por un producto interno en un espacio vectorial \\(V\\) con producto interno la función \\(||\\cdot||:V\\to \\mathbb{R}\\) definida por\n\\[||v||=\\sqrt{\\langle v,v \\rangle},\\] se llama norma inducida por el producto interno.\n\n\n\n\n\n\n\n\nTeorema\n\n\n\nSea \\(V\\) un espacio vectorial con producto interno, entonces la norma inducida por el producto interno satisface las siguientes propiedades: + \\(||v||\\geq 0\\) y \\(||v||=0\\) si y solo si \\(v=0\\). + \\(||\\lambda v||=|\\lambda|\\,||v||\\) para todo \\(\\lambda\\in \\mathbb{R}\\). + \\(||v+w||\\leq ||v||+||w||\\) para todo \\(v,w\\in V\\).\n\n\nRegularmente un espacio vectorial con producto interno se llama espacio prehilbertiano. Este tipo de espacios son muy importantes en el estudio de la física y la ingeniería. En este tipo de espacios podemos definir la noción de ortogonalidad, la cual consiste en que dos vectores son ortogonales si su producto interno es cero. Además, podemos definir si un conjunto de vectores es ortogonal si entre ellos son ortogonales dos a dos. Finalmente, podemos definir si un conjunto de vectores es ortonormal si son ortogonales y tienen norma uno.\n\n\n\n\n\n\nTeorema\n\n\n\nSea \\(W\\) un subconjunto finito de vectores ortogonales de un espacio vectorial con producto interno \\(V\\), entonces los elementos de \\(W\\) son linealmente independientes.\n\n\nimport requests from PIL import Image from io import BytesIO\nurl = “https://raw.githubusercontent.com/username/repository/branch/path/to/image.jpg”\nresponse = requests.get(url) image = Image.open(BytesIO(response.content))"
  },
  {
    "objectID": "solu_ecua.html",
    "href": "solu_ecua.html",
    "title": "4  Solución de sistema de ecuaciones lineales",
    "section": "",
    "text": "Supongamos que tenemos un sistema de ecuaciones lineales de la forma:\n\\[ \\begin{align*}\na_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n &= b_1 \\\\\na_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n &= b_2 \\\\\n\\vdots \\\\\na_{m1}x_1 + a_{m2}x_2 + \\cdots + a_{mn}x_n &= b_m \\\\\n\\end{align*} \\]\nnotemos que podemos escribirlo de forma matricial como:\n\\[ \\begin{pmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\    \na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n\\end{pmatrix} \\begin{pmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n \\\\\n\\end{pmatrix} = \\begin{pmatrix}\nb_1 \\\\\nb_2 \\\\\n\\vdots \\\\\nb_m \\\\\n\\end{pmatrix}, \\]\n\\[ \\begin{pmatrix}\nA_{\\cdot 1} & A_{\\cdot 2} & \\cdots & A_{\\cdot n} \\\\\n\\end{pmatrix} \\begin{pmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n \\\\\n\\end{pmatrix} = \\begin{pmatrix}\nb_1 \\\\\nb_2 \\\\\n\\vdots \\\\\nb_m \\\\\n\\end{pmatrix}. \\]\nLa notación \\(A_{\\cdot j}\\) indica la columna \\(j\\) de la matriz \\(A\\), o de forma más compacta como:\n\\[ \\sum _{j=1}^n A_{\\cdot j}x_j = \\begin{pmatrix}\nb_1 \\\\\nb_2 \\\\\n\\vdots \\\\\nb_m \\\\\n\\end{pmatrix}, \\]\nrecuerde que \\(x_j\\) es un escalar.\nAquí nos podemos preguntar si existe una solución para este sistema de ecuaciones, y si existe, ¿es única?. Para analizar esto, observemos que el sistema tiene solución si y solo si el vector \\(\\vec{b}\\) es combinación lineal de las columnas de la matriz \\(A\\), en otras palabras pertenece al espacio columna de \\(A\\). En caso contrario el sistema no tiene solución. Ahora podemos preguntarnos si la solución es única, para ellos existen dos posibilidades\n\nLas columnas de \\(A\\) son linealmente independientes, en este caso la solución es única.\nLas columnas de \\(A\\) son linealmente dependientes y \\(n<m\\), en este caso la solución no es única, esto quiere decir que \\(r<n\\).\nLas columnas de \\(A\\) son linealmente dependientes y \\(m<n\\).\n\nLa primera afirmación es una consecuencia de la definición de combinación lineal. En el segundo caso podemos ver que si las columnas de \\(A\\), son linealmente dependientes, entonces, sin perdida de generalidad, supondremos que las primeras \\(r\\) columnas son linealmente independientes, por tanto las restantes \\(n-r\\) columnas son combinación lineal de las primeras \\(r\\) columnas. Además las primeras \\(r\\) columnas son una base del espacio columna de \\(A\\), por tanto el vector \\(\\vec{b}\\) puede escribirse como combinación lineal de las primeras \\(r\\) columnas, así:\n\\[ \\vec{b} = \\sum_{j=1}^r \\tilde{\\alpha}_j A_{\\cdot j}, \\] lo cual quiere decir que un vector solución del sistema es:\n\\[ \\vec{x}_p=\\begin{pmatrix}\n\\tilde{\\alpha}_1 \\\\\n\\tilde{\\alpha}_2 \\\\\n\\vdots \\\\\n\\tilde{\\alpha}_r \\\\\n0 \\\\\n\\vdots \\\\\n0 \\\\\n\\end{pmatrix}, \\]\ndonde los últimos \\(m-r\\) elementos son cero. Ahora bien, queremos encontrar otro vector solución para ello estudiaremos la solución del sistema homogéneo asociado:\n\\[ A\\vec{x} = \\vec{0}, \\]\nrecuerde que \\(\\vec{x}\\in\\mathbb{R}^n\\) y \\(\\vec{0}\\in\\mathbb{R}^m\\). Este sistema tiene solución trivial. Sea \\(\\vec{x_h }=\\vec{x_1}+\\vec{x_2}\\), donde\n\\[ \\vec{x}_1=\\begin{pmatrix}\n\\alpha_1 \\\\\n\\alpha_2 \\\\\n\\vdots \\\\\n\\alpha_r \\\\\n0 \\\\\n\\vdots \\\\\n0 \\\\\n\\end{pmatrix}, \\text{ y } \\vec{x}_2=\\begin{pmatrix}\n0 \\\\\n0 \\\\\n\\vdots \\\\\n0 \\\\\n\\beta_{r+1} \\\\\n\\vdots \\\\\n\\beta_m \\\\\n\\end{pmatrix},\\]\ncalculando \\(A(\\vec{x}_1-\\vec{x}_2)\\) se obtiene que:\n\\[ \\sum_{j=1}^r \\alpha_j A_{\\cdot j} - \\sum_{j=r+1}^m \\beta_j A_{\\cdot j} = \\vec{0}, \\]\nluego para cualquier combinación de \\(\\beta_j\\) encontramos una combinación de \\(\\alpha_j\\) que la suma anterior sea cero, por tanto existen infinitas soluciones del sistema homogéneo asociado. Por tanto, la solución del sistema original es:\n\\[ \\vec{x} = \\vec{x}_p + \\vec{x}_h,\\] puesto \\[A(\\vec{x}_p + \\vec{x}_h) = A\\vec{x}_p + A\\vec{x}_h = \\vec{b} + \\vec{0} = \\vec{b}.\\]\n\n\n\n\n\n\nNote\n\n\n\nDe la demostración anterior podemos podemos ver que \\[ dim(N(A))=n-r. \\]\n\n\n\n\n\n\n\n\nEjercicio\n\n\n\n\n\n\nEl tercer caso es similar al segundo, la justificación se deja como ejercicio.\nMuestre que el \\(Rank(A^T)=Rank(A)\\).\nMuestre que el \\(dim((A^T))=m-r\\)."
  },
  {
    "objectID": "normas.html",
    "href": "normas.html",
    "title": "5  Norma de vectores",
    "section": "",
    "text": "6 Propiedades de las normas \\(l_p\\)\n\\[\\|x\\|_p\\geq 0\\text{ si }x\\neq 0\\]\ny \\(\\|x\\|_p=0\\) si y solo si \\(x=0\\).\n\\[\\| \\mathbf{v} + \\mathbf{w} \\|_p \\leq \\| \\mathbf{v} \\|_p + \\| \\mathbf{w} \\|_p\\]\n¿Cómo seria una circunferencia en la norma l_p?\nSi \\(p=2\\), la norma \\(L_2\\) se conoce como la norma Euclidiana o norma 2. La norma Euclidiana de un vector \\(x\\) se define como: \\[\\|x\\|_2=\\sqrt{\\sum_{i=1}^n|x_i|^2}\\] Ahora si \\(x\\) es un vector columna, la norma Euclidiana se puede escribir como: \\[\\|x\\|_2=\\sqrt{x^Tx}\\]\nEl número de condición de una matriz es una medida de la sensibilidad de la solución de un sistema de ecuaciones lineales \\(Ax = b\\) a pequeños cambios en la matriz \\(A\\). Esta definición es válida tanto para matrices cuadradas como rectangulares. El número de condición se denota \\(cond(A)\\), donde \\(A\\) es una matriz cuadrada o rectangular.\nEl número de condición de una matriz \\(A\\) se define como:\n\\[cond(A) = |\\|A\\|| \\cdot |\\|A^{-1}\\||\\]\nUna matriz se dice bien condicionada si su número de condición es cercano a 1.\nSea la matriz \\(A=\\begin{bmatrix} 1+10^{-4} & 1 \\\\ 1 & 1 \\end{bmatrix}\\), ¿Cúal es numero de condición de \\(A\\)?\nSupongamos que queremos resolver este sistema de ecuaciones\n\\[\\begin{bmatrix} 1+10^{-4} & 1 \\\\ 1 & 1 \\end{bmatrix}\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}=\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\]\n¿qué características podemos ver? ¿que tan sensibles es el sistema a pequeños cambios en la matriz \\(A\\)?\nEn un sistema de ecuaciones se refiere a cómo pequeñas variaciones en los coeficientes o términos del sistema afectan las soluciones o solución del sistema. En otras palabras, la sensibilidad se refiere a cuánto cambian las soluciones cuando los datos o las condiciones del problema cambian ligeramente."
  },
  {
    "objectID": "normas.html#normas-matriciales",
    "href": "normas.html#normas-matriciales",
    "title": "5  Norma de vectores",
    "section": "7.1 Normas matriciales",
    "text": "7.1 Normas matriciales\nLas normas matriciales inducidas son una forma de medir la magnitud de una matriz en relación con un espacio vectorial. Son útiles en diversas áreas de las matemáticas y la ciencia, incluyendo el álgebra lineal y la teoría de matrices. En este documento, exploraremos las normas matriciales inducidas y cómo se calculan.\nNorma Matricial Inducida por un Vector ** Propiedades\n\nSi \\(A\\) y \\(B\\) son matrices\n\n\\[|\\|AB\\||_p\\leq|\\|A|\\|_p\\||B\\||_p\\]\nLa norma matricial inducida por una norma vectorial se define como: \\[|\\|A\\||_{p} = \\max_{x \\neq 0} \\frac{\\|Ax\\|_{p}}{\\|x\\|_{q}}\\] donde \\(\\|\\cdot\\|_{p}\\) y \\(\\|\\cdot\\|_{q}\\) son normas vectoriales en \\(\\mathbb{R}^{n}\\) y \\(\\mathbb{R}^{m}\\), respectivamente. Esta definiciión se puede reescribir como:\n\\[|\\|A\\||_{p} = \\max_{\\|x\\|_{q} = 1} \\|Ax\\|_{p}\\]\nSi \\(A\\) es una matriz cuadrada, entonces la norma matricial de la norma \\(1\\) se puede escribir como: \\[|\\|A\\||_{1}=max_{1\\leq k\\leq n}\\sum_{j=1}^{n}|A_{j,k}|\\]\n\\[|\\|A\\||_{\\infty}=max_{1\\leq j\\leq n}\\sum_{k=1}^{n}|A_{j,k}|\\]"
  },
  {
    "objectID": "normas.html#ejemplos",
    "href": "normas.html#ejemplos",
    "title": "5  Norma de vectores",
    "section": "7.2 Ejemplos",
    "text": "7.2 Ejemplos\nVeamos algunos ejemplos de cálculo de normas matriciales inducidas en matrices y vectores específicos.\nEjemplo 1 Dada la matriz \\[A = \\begin{bmatrix} 2 & -1 \\\\ -5 & 4 \\end{bmatrix}\\] y la norma vectorial \\(L_{1}\\), calculemos la norma matricial inducida por \\(L_{1}\\).\n\\[|\\|A\\||_{1} = \\max\\{7,5\\}=7\\]\nNorma infinito\n\\[|\\|A\\||_{\\infty} = \\max\\{3,9\\}=9\\]"
  },
  {
    "objectID": "normas.html#normas-de-frobenius",
    "href": "normas.html#normas-de-frobenius",
    "title": "5  Norma de vectores",
    "section": "7.3 Normas de Frobenius",
    "text": "7.3 Normas de Frobenius\nLa norma de Frobenius es una norma matricial no inducida. La norma de Frobenius de una matriz \\(A\\) se define como:\n\\[|\\|A\\||_{F} = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} |a_{ij}|^{2}}\\]\n\nimport numpy as np\n\n# Crear una matriz de ejemplo\nA = np.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9]])\n\n# Calcular la norma de Frobenius (norma F)\nnorm_frobenius = np.linalg.norm(A, ord='fro')\nprint(\"Norma de Frobenius de A:\", norm_frobenius)\n\n# Calcular la norma infinito (norma ∞)\nnorm_inf = np.linalg.norm(A, ord=np.inf)\nprint(\"Norma infinito de A:\", norm_inf)\n\n# Calcular la norma 1 (norma 1)\nnorm_1 = np.linalg.norm(A, ord=1)\nprint(\"Norma 1 de A:\", norm_1)\n\nNorma de Frobenius de A: 16.881943016134134\nNorma infinito de A: 24.0\nNorma 1 de A: 18.0"
  },
  {
    "objectID": "gramsmith.html",
    "href": "gramsmith.html",
    "title": "9  Factorización QR",
    "section": "",
    "text": "10 Nota\nSi \\(A_{n\\times n}\\), donde cada una de sus columnas son linealmente idependientes entonces las columnas de \\(Q\\) forman una base para $^n $. Además la matriz \\(Q\\) es ortonormal, es decir \\(QQ^T=Q^TQ=I\\).\nLa factorización \\(QR\\) tiene muchas aplicaciones, las más relevantes son: soluciones de ecuaciones y mínimos cuadrados. Para mostrar la primera aplicación, trabajaremos con la interpolación de un polinomio de grado 7.\nSea los puntos \\(x_0=2.0\\), \\(x_1=1.2\\),…,\\(x_{10}=4.0\\), puntos igualmente espaciados entre el intervalo \\([2,4]\\) y sea el conjunto \\(y_i=1+x_i+x_1^2+\\cdots+x_i^{10}\\) para \\(0\\leq i\\leq 10\\). Obviamente el polinomio que que une a estos puntos es \\(P(x)=1+x+x^2+\\cdots+x^{10}\\). supongamos que no conocemos el polinomio \\(P(x)\\), y lo queremos encontrar, de esta forma podemos decir que \\[P(x)=c_0+c_1x+\\cdots+c_{10}x^{10},\\] de esta forma podemos plantear el sistemas de ecuaciones\n\\[\\begin{pmatrix}\n1&x_0&x_0^2 &  & x^{10}_{0}\\\\\n\\vdots&\\vdots & \\vdots & & \\vdots\\\\\n1&x_{10}&x_{10}^2 &  & x^{10}_{10}\n\\end{pmatrix}\\begin{pmatrix}c_0\\\\\\vdots\\\\c_{10}\\end{pmatrix}=\\begin{pmatrix}y_0\\\\\\vdots\\\\y_{10}\\end{pmatrix},\\] Note que este sistema tiene una solución la cual es el vector \\(c\\) que contiene los coeficientes del polinomio \\(P(x)\\). para resolver este problema vamos a usar la factorización \\(QR\\), para ello escribimos de forma matricial el sistema de ecuaciones anterior\n\\[Ac=y,\\] susituyendo \\(A=QR\\) tenemos que \\[QRc=y,\\] multiplicando por \\(Q^T\\) a ambos lados de la ecuación anterior tenemos que \\[Q^TQRc=Q^Ty,\\] como \\(Q^TQ=I\\) entonces \\[Rc=Q^Ty,\\] finalmente como \\(R\\) es una matriz triangular superior, podemos resolver el sistema de ecuaciones anterior de forma sencilla."
  },
  {
    "objectID": "gramsmith.html#pregunta",
    "href": "gramsmith.html#pregunta",
    "title": "9  Factorización QR",
    "section": "10.1 Pregunta",
    "text": "10.1 Pregunta\n¿Qué podemos decir de las filas de la matriz de \\(Q\\) si \\(A_{n\\times m}\\) con \\(n>m\\)? ¿ De los productos \\(Q^tQ\\) y \\(QQ^T\\)?"
  },
  {
    "objectID": "kmean.html",
    "href": "kmean.html",
    "title": "6  K-mean",
    "section": "",
    "text": "K-mean es un algoritmo de aprendizaje no supervisado que agrupa los datos en K grupos distintos. El número de grupos K es un parámetro que se debe especificar por el usuario. El algoritmo funciona iterativamente para asignar cada dato a uno de los K grupos basado en las características que se proporcionan. Los datos se agrupan en función de la similitud de sus características.\nPara realizar esta labor, el algoritmo K-mean utiliza la distancia euclidiana para asignar cada dato a un grupo, con el objetivo de que la suma de las distancias al cuadrado dentro de cada grupo sea mínima."
  },
  {
    "objectID": "regresion.html",
    "href": "regresion.html",
    "title": "11  Regresión multivariada",
    "section": "",
    "text": "12 Ejemplo 3\nLinealización para encontrar una relación entra la altura y el peso usando la lew de potencias\n\\[y=c_1e^{c_2t}\\] peso versus altura\nEl modelo de la ecuación \\[y=c_1te^{c_2t}\\] se puede utilizar para ajustar los datos de la concentración de un medicamento en la sangre de un paciente.\nModelos La temperatura en Washington, D.C., desde enero 1 de 2001 es listada en la siguiente tabla.\nSuponemos que el modelo se comporta de la forma\n\\[y=c_1+c_2\\cos(2\\pi t)+c_3\\sin(2\\pi t)\\] use mínimos cuadrados para encontrar los parámetros \\(c_i\\)\ndatos disponibles es datos"
  },
  {
    "objectID": "regresion.html#ejemplo",
    "href": "regresion.html#ejemplo",
    "title": "11  Regresión multivariada",
    "section": "11.1 Ejemplo",
    "text": "11.1 Ejemplo\nUn estudio quiere generar un modelo que permita predecir la esperanza de vida media de los habitantes de una ciudad en función de diferentes variables. Se dispone de información sobre la esperanza de vida media de los habitantes de 50 ciudades, junto con información sociodemográfica de cada una de ellas. En concreto, se conoce: el número de habitantes, nivel de analfabetismo, ingresos, asesinatos, cantidad de universitarios, heladas, área y densidad poblacional.\n\nimport pandas as pd\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/'\n    'Estadistica-machine-learning-python/master/data/state_x77.csv'\n)\ndatos = pd.read_csv(url, sep=',')\ndisplay(datos.info())\ndatos.head(3)\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 50 entries, 0 to 49\nData columns (total 9 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   habitantes      50 non-null     int64  \n 1   ingresos        50 non-null     int64  \n 2   analfabetismo   50 non-null     float64\n 3   esp_vida        50 non-null     float64\n 4   asesinatos      50 non-null     float64\n 5   universitarios  50 non-null     float64\n 6   heladas         50 non-null     int64  \n 7   area            50 non-null     int64  \n 8   densidad_pobl   50 non-null     float64\ndtypes: float64(5), int64(4)\nmemory usage: 3.6 KB\n\n\nNone\n\n\n\n\n\n\n  \n    \n      \n      habitantes\n      ingresos\n      analfabetismo\n      esp_vida\n      asesinatos\n      universitarios\n      heladas\n      area\n      densidad_pobl\n    \n  \n  \n    \n      0\n      3615\n      3624\n      2.1\n      69.05\n      15.1\n      41.3\n      20\n      50708\n      71.290526\n    \n    \n      1\n      365\n      6315\n      1.5\n      69.31\n      11.3\n      66.7\n      152\n      566432\n      0.644384\n    \n    \n      2\n      2212\n      4530\n      1.8\n      70.55\n      7.8\n      58.1\n      15\n      113417\n      19.503249\n    \n  \n\n\n\n\n\n\n\n\n\n\nEjercicios\n\n\n\n\n\nRespondan las siguientes preguntas:\n\n¿Podríamos encontrar una relación entre independencia lineal de dos variables y su correlación?\nHaga la regresión lineal con habitantes, asesinatos y densidad poblacional como variables explicativas. ¿Que puede decir de los valores de \\(\\beta\\)?\nEncuentre el valor del error cuadrático medio."
  },
  {
    "objectID": "regresion.html#linealización-de-modelos-no-lineales",
    "href": "regresion.html#linealización-de-modelos-no-lineales",
    "title": "11  Regresión multivariada",
    "section": "11.2 Linealización de modelos no lineales",
    "text": "11.2 Linealización de modelos no lineales\nEn ocasiones, los modelos no siguen una relación lineal entre las variables explicativas y la variable de respuesta. En estos casos, se puede linealizar el modelo, es decir, transformar el modelo no lineal en uno lineal. Por ejemplo, si se tiene un modelo de la forma: El crecimiento de una población de bacterias sigue una ley exponencial, es decir, el número de bacterias en el tiempo \\(t\\) es de la forma:\n\\[N(t) = N_0 e^{rt},\\]\ndonde \\(N_0\\) es el número inicial de bacterias, \\(r\\) es la tasa de crecimiento y \\(t\\) es el tiempo. Si tomamos logaritmos en ambos lados de la ecuación, obtenemos:\n\\[\\log(N(t)) = \\log(N_0) + rt,\\]\nNote de esta forma podemos lienalizar el modelo."
  },
  {
    "objectID": "regresion.html#ejemplo-2",
    "href": "regresion.html#ejemplo-2",
    "title": "11  Regresión multivariada",
    "section": "11.3 Ejemplo 2",
    "text": "11.3 Ejemplo 2\nLa ecuación que mas aproxima el comportamiento de la venta de carros es \\[y=c_1e^{c_2t}\\] Podemos lienalizar este modelo para que usando mínimos cuadrados encontremos los parametros \\(c_1\\) y \\(c_2\\) que mejor ajustan el modelo a los datos\n\nimport xml.etree.ElementTree as ET\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndf = pd.read_csv('datos/cpu.csv')\nprint(df.head())\nplt.plot(df['transistors'],'ro')\n\n    CPU  year  transistors\n0  4004  1971         2250\n1  8008  1972         2500\n2  8080  1974         5000\n3  8086  1978        29000\n4   286  1982       120000"
  }
]