[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Algebra lineal Computacional",
    "section": "",
    "text": "1 Prefacio\nEstas son las notas de la clase de Algebra lineal computacional, en ningún momento pretenden ser un libro de texto, ni mucho menos un sustituto de las clases. Son un apoyo para los estudiantes que deseen repasar los temas vistos en clase."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2  Introducción",
    "section": "",
    "text": "En álgebra lineal computacional es una herramienta que esta ne auge en la actualidad, ya que permite resolver diversos problemas y va de la mano con la eficiencia computacional. En este curso haremos una pequeña mirada al álgebra lineal computacional y resolveremos diferentes aplicaciones usando el lenguaje de programación Python. la primera parte de este curso se usará el libro Strang (2022) como guía. En el segunda parte del curso usaremos Sauer (2018) como guía.\n\n\n\n\nSauer, T. 2018. Numerical Analysis. Pearson Education. https://books.google.com.co/books?id=gAVTDwAAQBAJ.\n\n\nStrang, Gilbert. 2022. Introduction to Linear Algebra. SIAM."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Sauer, T. 2018. Numerical Analysis. Pearson Education. https://books.google.com.co/books?id=gAVTDwAAQBAJ.\n\n\nStrang, Gilbert. 2022. Introduction to Linear Algebra. SIAM."
  },
  {
    "objectID": "summary.html#ejemplos",
    "href": "summary.html#ejemplos",
    "title": "3  Espacios vectoriales",
    "section": "3.1 Ejemplos",
    "text": "3.1 Ejemplos\n\n\\(\\mathcal{R}̣\\)\n\\(\\mathcal{C}\\)\n\\(\\mathcal{R}^n\\)\n\\(\\mathcal{C}^n\\)\nEl conjunto de todos los vectores ortogonales a un vector dado\nEl conjunto de todas las funciones definidas en \\(\\mathcal{R}\\)\nEl conjunto de todas las funciones continuas en un intervalo dado\nEl conjunto de todas las matrices del mismo tamaño\nEl conjunto de todas las funciones diferenciables en un intervalo dado\nEl conjunto de todas las funciones que satisfacen la ecuación diferencial \\[y''+ay'+by.\\]\n\n\n3.1.1 Subespacios de un espacio vectorial\nSea \\(V\\) un espacio vectorial y \\(S\\) un subconjunto de \\(V\\), \\(S\\) es un subespacio de \\(V\\), si en si mismo es un espacio vectorial.\n\n\n\n\n\n\nTeorema\n\n\n\nSea \\(S\\) un subconjunto no vació de un espacio vectorial \\(V\\). Si los elementos de \\(S\\) satisfacen los axiomas de clausura entonces \\(S\\) es un subespacio de \\(V\\)\n\n\n\n\n\n\n\n\nDefinición\n\n\n\nSea \\(S\\) un subconjunto no vació de un espacio vectorial \\(V\\), un elemento \\(x\\) de \\(V\\) de la forma: \\[x=\\sum_{i=1}^{k}c_ix_i,\\] en donde \\(x_1,x_2,...,x_k\\) son elementos de \\(S\\) y \\(c_i\\) son escalares, se denomina la combinación lineal de elementos de \\(S\\).\n\n\n\n\n\n\n\n\nTeorema\n\n\n\nEl conjunto \\(span(S)=\\{\\sum_{i=1}^{k}c_ix_i,|c_i\\in F\\}\\) de todos los elementos generados en combinación lineal de \\(S\\) es un subespacio vectorial de \\(V\\)\n\n\n\n\n\n\n\n\nNote\n\n\n\nSi \\(S\\) es un subespacio de \\(V\\) entonces \\(S=span(S)\\). Además, el generado \\(gen(S)=span(S)\\)\n\n\nSupongamos que queremos calcular el \\(span(S)\\) donde \\(S\\{x_1,x_2,...,x_n\\}\\), y \\(x_i\\in\\mathbb{R^n}\\), para esto debemos calcular todas las combinaciones lineales de los elementos de \\(S\\) de la siguinete forma\n\\[span(S)=\\{\\sum_{i=1}^{n}c_ix_i,|c_i\\in\\mathbb{R}\\}\\]\nOtra forma de calcular esta combinación es crear una matriz \\(M\\) con todos los vectores de \\(S\\) y multiplicarla por un vector columna de los coeficientes \\(c_i\\), de esta forma obtenemos un vector columna con todos los elementos de la combinación lineal, esto se puedo expresar como la multiplicación de una matriz por un vector columna.\n\\[span(S)=M\\in\\mathbb{R}c\\]\n\n\n\n\n\n\nDefinición\n\n\n\nSea \\(A\\) una matriz de \\(m\\times n\\), definimos como \\(Im(A)=span\\{y\\in\\mathbb{R^m}|Ax=y\\}\\), note si tomamos \\(x=e_i\\) tenemos donde \\(Ae_i=A_i\\) son las columnas de \\(A\\). Además definimos \\(Ker(A)=\\{x\\in\\mathbb{R^n}|Ax=0\\}\\)."
  },
  {
    "objectID": "subespacios.html",
    "href": "subespacios.html",
    "title": "6  Subespacios principales de una matriz",
    "section": "",
    "text": "Una matriz \\(A_{m\\times n}\\) crea cuatro subespacios importantes, como los podemos mostrar en la siguiente figura. (Aquí se puede encontrar màs inforacion del tema)\n\n\n\n4 espacios fundamentales\n\n\nLlamaremos \\(C(A)\\) como el espacio columna de la matriz \\(A\\) y \\(N(A)\\) como el espacio nulo de la matriz de \\(A\\). De aquì podemos observar las siguientes propiedades\n\\[C(A^T)\\perp N(A).\\] Para mostrar esta proiedad debemos mostrar que si \\(x_1\\in C(A^T)\\) y \\(x_2\\in N(A)\\), entonces\n\\[\\langle x_1,x_2\\rangle=x_2^Tx_1=0.\\]\nSabemos que existe un \\(y\\in \\mathbb{R}^m\\), note que lo podemos descomponer en \\(y=y_1+y_2\\), donde \\(y_1\\in C(A)\\) y \\(y_2\\in N(A^T)\\) luego \\[A^Ty=A^T(y_1+y_2)=A^ty_1=x_1.\\]\nAhora tenemos que\n\\[\\langle x_2,x_1\\rangle=(A^Ty)^Tx_2=y^TAx_2=0,\\] por tanto son ortogonales. Con un argumento similar podemos mostrar que \\(C(A)\\perp N(A^T)\\).\nAdemas tenemos la propiedad que \\[dim(C(A))=dim(N(A^T))=r,\\] de esta forma tenemos que \\(dim(N(A))=n-r\\) y \\(dim(N(A^T))=m-r\\).\n\n\n\n\n\n\nDefinición\n\n\n\nSea \\(A\\) una matriz de \\(m\\times n\\), definimos como el \\(rank(A)\\) como el numero máximo de filas o columnas lienalmente independientes de \\(A\\). Note que por la propiedad anterior tenemos que \\(rank(A)=dim(C(A))=dim(N(A^T))\\).\n\n\nAlgunas propiedades importantes del rank son las siguientes\n\nSi \\(A\\) es una matriz cuadrada y \\(rank(A)=n\\) entonces todos autovalores de \\(A\\) son distintos de cero.\nSi \\(A\\) es una matriz cuadrada entonces \\(rank(A)=n\\) si y solo si \\(A\\) es invertible.\n\nPara demostrar la primera vamos a suponer que es falso por tanto 0 es un autovalor luego \\(Ax=0\\), lo que implica que \\(N(A)\\neq\\emptyset\\), esto contradice el hecho que \\(rank(A)=dim(C(A^T))=dim(C(A))=n\\). Ahora, si \\(A\\) tiene todos los autovalores diferentes de cero entonces \\(A\\) es invertible, por lo tanto \\(rank(A)=n\\).\n\nSi \\(A\\) una matriz de \\(m\\times n\\), con \\(m>n\\) entonces \\(A^TA\\) es invertible.\n\nSea \\(x\\in \\mathbb{R}^n\\) entonces \\(Ax=y\\neq 0\\) puesto que \\(rank(A)=n,\\) entonces \\(y\\notin N(A^T)\\). De esta forma existe un \\(x\\neq 0\\) tal que \\(x=A^Ty\\) y que \\(x\\in C(A^T)\\) y \\(x\\neq 0\\). Para completar la demostración debemos debemos mostrar que \\(dim(Im(A^TA))=n\\).\nPara esto vamos a usar la propiedad que \\(rank(A)=n\\) y \\(rank(A^T)=n\\). Entonces tenemos que \\(rank(A^TA)=n\\) y por lo tanto \\(A^TA\\) es invertible."
  },
  {
    "objectID": "espacios_vectoriales.html#ejemplos",
    "href": "espacios_vectoriales.html#ejemplos",
    "title": "3  Espacios vectoriales",
    "section": "3.1 Ejemplos",
    "text": "3.1 Ejemplos\n\n\\(\\mathcal{R}̣\\)\n\\(\\mathcal{C}\\)\n\\(\\mathcal{R}^n\\)\n\\(\\mathcal{C}^n\\)\nEl conjunto de todos los vectores ortogonales a un vector dado\nEl conjunto de todas las funciones definidas en \\(\\mathcal{R}\\)\nEl conjunto de todas las funciones continuas en un intervalo dado\nEl conjunto de todas las matrices del mismo tamaño\nEl conjunto de todas las funciones diferenciables en un intervalo dado\nEl conjunto de todas las funciones que satisfacen la ecuación diferencial \\[y''+ay'+by.\\]\n\n\n3.1.1 Subespacios de un espacio vectorial\nSea \\(V\\) un espacio vectorial y \\(S\\) un subconjunto de \\(V\\), \\(S\\) es un subespacio de \\(V\\), si en si mismo es un espacio vectorial.\n\n\n\n\n\n\nTeorema\n\n\n\nSea \\(S\\) un subconjunto no vació de un espacio vectorial \\(V\\). Si los elementos de \\(S\\) satisfacen los axiomas de clausura entonces \\(S\\) es un subespacio de \\(V\\)\n\n\n\n\n\n\n\n\nDefinición\n\n\n\nSea \\(S\\) un subconjunto no vació de un espacio vectorial \\(V\\), un elemento \\(x\\) de \\(V\\) de la forma: \\[x=\\sum_{i=1}^{k}c_ix_i,\\] en donde \\(x_1,x_2,...,x_k\\) son elementos de \\(S\\) y \\(c_i\\) son escalares, se denomina la combinación lineal de elementos de \\(S\\).\n\n\n\n\n\n\n\n\nTeorema\n\n\n\nEl conjunto \\(span(S)=\\{\\sum_{i=1}^{k}c_ix_i,|c_i\\in F\\}\\) de todos los elementos generados en combinación lineal de \\(S\\) es un subespacio vectorial de \\(V\\)\n\n\n\n\n\n\n\n\nNote\n\n\n\nSi \\(S\\) es un subespacio de \\(V\\) entonces \\(S=span(S)\\). Además, el generado \\(gen(S)=span(S)\\)\n\n\nSupongamos que queremos calcular el \\(span(S)\\) donde \\(S\\{x_1,x_2,...,x_n\\}\\), y \\(x_i\\in\\mathbb{R^n}\\), para esto debemos calcular todas las combinaciones lineales de los elementos de \\(S\\) de la siguiente forma\n\\[span(S)=\\{\\sum_{i=1}^{n}c_ix_i,|c_i\\in\\mathbb{R}\\}\\]\nOtra forma de calcular esta combinación es crear una matriz \\(M\\) con todos los vectores de \\(S\\) y multiplicarla por un vector columna de los coeficientes \\(c_i\\), de esta forma obtenemos un vector columna con todos los elementos de la combinación lineal, esto se puedo expresar como la multiplicación de una matriz por un vector columna.\n\\[span(S)=M\\in\\mathbb{R}c\\]\n\n\n\n\n\n\nDefinición\n\n\n\nSea \\(A\\) una matriz de \\(m\\times n\\), definimos como el espacio columna de \\(A\\)\n\\[C(A)=span\\{y\\in\\mathbb{R^m}|Ax=y\\},\\]\ny este se denota por \\(C(A)\\).\nNote si tomamos \\(x=e_i\\) tenemos donde \\(Ae_i=A_{\\cdot i}\\) es \\(i-esima\\) columna de \\(A\\). Además definimos como el espacio nulo de \\(A\\)\n\\[N(A)=span\\{x\\in\\mathbb{R^n}|Ax=0\\}.\\]\n\n\n\n\n\n\n\n\nDefinición\n\n\n\nSe \\(S=\\{x_1,x_2,...,x_n\\}\\) un conjunto de vectores linealmente independientes de \\(V\\), entonces se dice que \\(S\\) es una base de \\(V\\) si genera a \\(V\\).\n\n\nSi el conjunto \\(S\\) es finito, entonces se dice que \\(V\\) es de dimensión finita y se denota por \\(dim(V)=n\\). Existen espacios con dimensión infinita, pero no los estudiaremos en este curso.\n\n\n\n\n\n\nDefinición\n\n\n\nEl rank de una matriz \\(A\\) es la dimensión del espacio columna de \\(A\\) y se denota por \\(rank(A)\\).\n\n\n\n\n\n\n\n\nDefinición\n\n\n\nSea \\(A\\) una matriz de \\(m\\times n\\), con \\(m>n\\) \\(rank(A)=n\\) diremos que \\(A\\) es una matriz de rango completo.\n\n\n\n\n\n\n\n\nEjercicio\n\n\n\n\n\n\nMuestre que si una matriz \\(A\\) todas sus columnas son linealmente independientes entonces el espacio nulo de \\(A\\) es el vector \\(\\{0\\}\\).\nSi \\(A\\) es una matriz de \\(m\\times n\\) y \\(B\\) es una matriz de \\(n\\times p\\) muestre que el espacio nulo de \\(AB\\) es un subespacio del espacio nulo de \\(B\\).\nSi \\(A\\) es una matriz de \\(n\\times n\\) y la dimensión de su espacio columna es \\(n\\) entonces la inversa de \\(A\\) existe."
  },
  {
    "objectID": "espacios_vectoriales.html",
    "href": "espacios_vectoriales.html",
    "title": "3  Espacios vectoriales",
    "section": "",
    "text": "4 Solución de sistema de ecuaciones lineales"
  },
  {
    "objectID": "espacios_vectoriales.html#ejercicio",
    "href": "espacios_vectoriales.html#ejercicio",
    "title": "3  Espacios vectoriales",
    "section": "3.2 Ejercicio",
    "text": "3.2 Ejercicio\nMuestre que si una matriz \\(A\\) todas sus columnas son linealmente independientes entonces el espacio nulo de \\(A\\) es el vector \\(\\{0\\}\\). #:::"
  },
  {
    "objectID": "espacios_vectoriales.html#ejercicio-1",
    "href": "espacios_vectoriales.html#ejercicio-1",
    "title": "3  Espacios vectoriales",
    "section": "3.2 Ejercicio",
    "text": "3.2 Ejercicio\n#:::"
  },
  {
    "objectID": "producto.html",
    "href": "producto.html",
    "title": "6  Producto interno en un espacio vectorial",
    "section": "",
    "text": "7 Do something with the image\nimage.show()"
  },
  {
    "objectID": "producto.html#ejemplos",
    "href": "producto.html#ejemplos",
    "title": "6  Producto interno en un espacio vectorial",
    "section": "6.1 Ejemplos",
    "text": "6.1 Ejemplos\n\nEl producto punto en \\(\\mathbb{R}^n\\) es un producto interno.\nSea \\(A\\) una matriz simétrica definida positiva en \\(\\mathbb{R}^n\\) define un producto interno.\nSea \\(C^n\\) el espacio de todas las funciones continuas en el intervalo \\([a,b]\\). Entonces \\(f,g\\in C^n\\) se define el producto interno como \\[\\int_a^b f(x)g(x)dx.\\]\n\n\n\n\n\n\n\nTeorema\n\n\n\nEn un espacio vectorial con producto interno \\(V\\), todo producto interno satisfacen la desigualdad de Cauchy-Schwarz, es decir, para todo \\(v,w\\in V\\) se tiene que \\[|\\langle v,w \\rangle|^2\\leq \\langle v,v \\rangle\\langle w,w \\rangle.\\]\n\n\n\n\n\n\n\n\nDemostración\n\n\n\n\n\nSea \\(v,w\\in V\\) y \\(\\lambda\\in \\mathbb{R}\\), entonces, si \\(w=0\\) o \\(v=0\\) se tiene que la desigualdad se cumple trivialmente. Supongamos que \\(v\\) y \\(w\\) ambos no son cero. Sea \\(z=av+bw\\) con \\(a,b\\in \\mathbb{R}\\), entonces\n\\[0\\leq \\langle z,z \\rangle=\\langle av+bw,av+bw \\rangle=a^2\\langle v,v \\rangle+ab\\langle v,w \\rangle+ba\\langle w,v \\rangle+b^2\\langle w,w \\rangle,\\] tomando \\(a=\\langle w,w \\rangle\\) y \\(b=-\\langle v,w \\rangle,\\) se tiene que \\[0\\leq \\langle w,w \\rangle^2\\langle v,v \\rangle-2\\langle v,w \\rangle\\langle w,w \\rangle\\langle v,w \\rangle+\\langle v,w \\rangle^2\\langle w,w \\rangle=\\langle w,w \\rangle\\langle v,v \\rangle-\\langle v,w \\rangle^2,\\] de donde se sigue que \\[\\langle v,w \\rangle^2\\leq \\langle v,v \\rangle\\langle w,w \\rangle.\\]\n\n\n\n\n\n\n\n\n\nDefinición\n\n\n\nNorma inducida por un producto interno en un espacio vectorial \\(V\\) con producto interno la función \\(||\\cdot||:V\\to \\mathbb{R}\\) definida por\n\\[||v||=\\sqrt{\\langle v,v \\rangle},\\] se llama norma inducida por el producto interno.\n\n\n\n\n\n\n\n\nTeorema\n\n\n\nSea \\(V\\) un espacio vectorial con producto interno, entonces la norma inducida por el producto interno satisface las siguientes propiedades: + \\(||v||\\geq 0\\) y \\(||v||=0\\) si y solo si \\(v=0\\). + \\(||\\lambda v||=|\\lambda|\\,||v||\\) para todo \\(\\lambda\\in \\mathbb{R}\\). + \\(||v+w||\\leq ||v||+||w||\\) para todo \\(v,w\\in V\\).\n\n\nRegularmente un espacio vectorial con producto interno se llama espacio prehilbertiano. Este tipo de espacios son muy importantes en el estudio de la física y la ingeniería. En este tipo de espacios podemos definir la noción de ortogonalidad, la cual consiste en que dos vectores son ortogonales si su producto interno es cero. Además, podemos definir si un conjunto de vectores es ortogonal si entre ellos son ortogonales dos a dos. Finalmente, podemos definir si un conjunto de vectores es ortonormal si son ortogonales y tienen norma uno.\n\n\n\n\n\n\nTeorema\n\n\n\nSea \\(W\\) un subconjunto finito de vectores ortogonales de un espacio vectorial con producto interno \\(V\\), entonces los elementos de \\(W\\) son linealmente independientes.\n\n\nimport requests from PIL import Image from io import BytesIO\nurl = “https://raw.githubusercontent.com/username/repository/branch/path/to/image.jpg”\nresponse = requests.get(url) image = Image.open(BytesIO(response.content))"
  },
  {
    "objectID": "solu_ecua.html",
    "href": "solu_ecua.html",
    "title": "4  Solución de sistema de ecuaciones lineales",
    "section": "",
    "text": "Supongamos que tenemos un sistema de ecuaciones lineales de la forma:\n\\[ \\begin{align*}\na_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n &= b_1 \\\\\na_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n &= b_2 \\\\\n\\vdots \\\\\na_{m1}x_1 + a_{m2}x_2 + \\cdots + a_{mn}x_n &= b_m \\\\\n\\end{align*} \\]\nnotemos que podemos escribirlo de forma matricial como:\n\\[ \\begin{pmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\    \na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n\\end{pmatrix} \\begin{pmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n \\\\\n\\end{pmatrix} = \\begin{pmatrix}\nb_1 \\\\\nb_2 \\\\\n\\vdots \\\\\nb_m \\\\\n\\end{pmatrix}, \\]\n\\[ \\begin{pmatrix}\nA_{\\cdot 1} & A_{\\cdot 2} & \\cdots & A_{\\cdot n} \\\\\n\\end{pmatrix} \\begin{pmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n \\\\\n\\end{pmatrix} = \\begin{pmatrix}\nb_1 \\\\\nb_2 \\\\\n\\vdots \\\\\nb_m \\\\\n\\end{pmatrix}. \\]\nLa notación \\(A_{\\cdot j}\\) indica la columna \\(j\\) de la matriz \\(A\\), o de forma más compacta como:\n\\[ \\sum _{j=1}^n A_{\\cdot j}x_j = \\begin{pmatrix}\nb_1 \\\\\nb_2 \\\\\n\\vdots \\\\\nb_m \\\\\n\\end{pmatrix}, \\]\nrecuerde que \\(x_j\\) es un escalar.\nAquí nos podemos preguntar si existe una solución para este sistema de ecuaciones, y si existe, ¿es única?. Para analizar esto, observemos que el sistema tiene solución si y solo si el vector \\(\\vec{b}\\) es combinación lineal de las columnas de la matriz \\(A\\), en otras palabras pertenece al espacio columna de \\(A\\). En caso contrario el sistema no tiene solución. Ahora podemos preguntarnos si la solución es única, para ellos existen dos posibilidades\n\nLas columnas de \\(A\\) son linealmente independientes, en este caso la solución es única.\nLas columnas de \\(A\\) son linealmente dependientes y \\(n<m\\), en este caso la solución no es única, esto quiere decir que \\(r<n\\).\nLas columnas de \\(A\\) son linealmente dependientes y \\(m<n\\).\n\nLa primera afirmación es una consecuencia de la definición de combinación lineal. En el segundo caso podemos ver que si las columnas de \\(A\\), son linealmente dependientes, entonces, sin perdida de generalidad, supondremos que las primeras \\(r\\) columnas son linealmente independientes, por tanto las restantes \\(n-r\\) columnas son combinación lineal de las primeras \\(r\\) columnas. Además las primeras \\(r\\) columnas son una base del espacio columna de \\(A\\), por tanto el vector \\(\\vec{b}\\) puede escribirse como combinación lineal de las primeras \\(r\\) columnas, así:\n\\[ \\vec{b} = \\sum_{j=1}^r \\tilde{\\alpha}_j A_{\\cdot j}, \\] lo cual quiere decir que un vector solución del sistema es:\n\\[ \\vec{x}_p=\\begin{pmatrix}\n\\tilde{\\alpha}_1 \\\\\n\\tilde{\\alpha}_2 \\\\\n\\vdots \\\\\n\\tilde{\\alpha}_r \\\\\n0 \\\\\n\\vdots \\\\\n0 \\\\\n\\end{pmatrix}, \\]\ndonde los últimos \\(m-r\\) elementos son cero. Ahora bien, queremos encontrar otro vector solución para ello estudiaremos la solución del sistema homogéneo asociado:\n\\[ A\\vec{x} = \\vec{0}, \\]\nrecuerde que \\(\\vec{x}\\in\\mathbb{R}^n\\) y \\(\\vec{0}\\in\\mathbb{R}^m\\). Este sistema tiene solución trivial. Sea \\(\\vec{x_h }=\\vec{x_1}+\\vec{x_2}\\), donde\n\\[ \\vec{x}_1=\\begin{pmatrix}\n\\alpha_1 \\\\\n\\alpha_2 \\\\\n\\vdots \\\\\n\\alpha_r \\\\\n0 \\\\\n\\vdots \\\\\n0 \\\\\n\\end{pmatrix}, \\text{ y } \\vec{x}_2=\\begin{pmatrix}\n0 \\\\\n0 \\\\\n\\vdots \\\\\n0 \\\\\n\\beta_{r+1} \\\\\n\\vdots \\\\\n\\beta_m \\\\\n\\end{pmatrix},\\]\ncalculando \\(A(\\vec{x}_1-\\vec{x}_2)\\) se obtiene que:\n\\[ \\sum_{j=1}^r \\alpha_j A_{\\cdot j} - \\sum_{j=r+1}^m \\beta_j A_{\\cdot j} = \\vec{0}, \\]\nluego para cualquier combinación de \\(\\beta_j\\) encontramos una combinación de \\(\\alpha_j\\) que la suma anterior sea cero, por tanto existen infinitas soluciones del sistema homogéneo asociado. Por tanto, la solución del sistema original es:\n\\[ \\vec{x} = \\vec{x}_p + \\vec{x}_h,\\] puesto \\[A(\\vec{x}_p + \\vec{x}_h) = A\\vec{x}_p + A\\vec{x}_h = \\vec{b} + \\vec{0} = \\vec{b}.\\]\n\n\n\n\n\n\nNote\n\n\n\nDe la demostración anterior podemos podemos ver que \\[ dim(N(A))=n-r. \\]\n\n\n\n\n\n\n\n\nEjercicio\n\n\n\n\n\n\nEl tercer caso es similar al segundo, la justificación se deja como ejercicio.\nMuestre que el \\(Rank(A^T)=Rank(A)\\).\nMuestre que el \\(dim((A^T))=m-r\\)."
  },
  {
    "objectID": "normas.html",
    "href": "normas.html",
    "title": "5  Norma de vectores",
    "section": "",
    "text": "6 Propiedades de las normas \\(l_p\\)\n\\[\\|x\\|_p\\geq 0\\text{ si }x\\neq 0\\]\ny \\(\\|x\\|_p=0\\) si y solo si \\(x=0\\).\n\\[\\| \\mathbf{v} + \\mathbf{w} \\|_p \\leq \\| \\mathbf{v} \\|_p + \\| \\mathbf{w} \\|_p\\]\n¿Cómo seria una circunferencia en la norma l_p?\nSi \\(p=2\\), la norma \\(L_2\\) se conoce como la norma Euclidiana o norma 2. La norma Euclidiana de un vector \\(x\\) se define como: \\[\\|x\\|_2=\\sqrt{\\sum_{i=1}^n|x_i|^2}\\] Ahora si \\(x\\) es un vector columna, la norma Euclidiana se puede escribir como: \\[\\|x\\|_2=\\sqrt{x^Tx}\\]\nEl número de condición de una matriz es una medida de la sensibilidad de la solución de un sistema de ecuaciones lineales \\(Ax = b\\) a pequeños cambios en la matriz \\(A\\). Esta definición es válida tanto para matrices cuadradas como rectangulares. El número de condición se denota \\(cond(A)\\), donde \\(A\\) es una matriz cuadrada o rectangular.\nEl número de condición de una matriz \\(A\\) se define como:\n\\[cond(A) = |\\|A\\|| \\cdot |\\|A^{-1}\\||\\]\nUna matriz se dice bien condicionada si su número de condición es cercano a 1.\nSea la matriz \\(A=\\begin{bmatrix} 1+10^{-4} & 1 \\\\ 1 & 1 \\end{bmatrix}\\), ¿Cúal es numero de condición de \\(A\\)?\nSupongamos que queremos resolver este sistema de ecuaciones\n\\[\\begin{bmatrix} 1+10^{-4} & 1 \\\\ 1 & 1 \\end{bmatrix}\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}=\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\\]\n¿qué características podemos ver? ¿que tan sensibles es el sistema a pequeños cambios en la matriz \\(A\\)?\nEn un sistema de ecuaciones se refiere a cómo pequeñas variaciones en los coeficientes o términos del sistema afectan las soluciones o solución del sistema. En otras palabras, la sensibilidad se refiere a cuánto cambian las soluciones cuando los datos o las condiciones del problema cambian ligeramente."
  },
  {
    "objectID": "normas.html#normas-matriciales",
    "href": "normas.html#normas-matriciales",
    "title": "5  Norma de vectores",
    "section": "7.1 Normas matriciales",
    "text": "7.1 Normas matriciales\nLas normas matriciales inducidas son una forma de medir la magnitud de una matriz en relación con un espacio vectorial. Son útiles en diversas áreas de las matemáticas y la ciencia, incluyendo el álgebra lineal y la teoría de matrices. En este documento, exploraremos las normas matriciales inducidas y cómo se calculan.\nNorma Matricial Inducida por un Vector ** Propiedades\n\nSi \\(A\\) y \\(B\\) son matrices\n\n\\[|\\|AB\\||_p\\leq|\\|A|\\|_p\\||B\\||_p\\]\nLa norma matricial inducida por una norma vectorial se define como: \\[|\\|A\\||_{p} = \\max_{x \\neq 0} \\frac{\\|Ax\\|_{p}}{\\|x\\|_{q}}\\] donde \\(\\|\\cdot\\|_{p}\\) y \\(\\|\\cdot\\|_{q}\\) son normas vectoriales en \\(\\mathbb{R}^{n}\\) y \\(\\mathbb{R}^{m}\\), respectivamente. Esta definiciión se puede reescribir como:\n\\[|\\|A\\||_{p} = \\max_{\\|x\\|_{q} = 1} \\|Ax\\|_{p}\\]\nSi \\(A\\) es una matriz cuadrada, entonces la norma matricial de la norma \\(1\\) se puede escribir como: \\[|\\|A\\||_{1}=max_{1\\leq k\\leq n}\\sum_{j=1}^{n}|A_{j,k}|\\]\n\\[|\\|A\\||_{\\infty}=max_{1\\leq j\\leq n}\\sum_{k=1}^{n}|A_{j,k}|\\]"
  },
  {
    "objectID": "normas.html#ejemplos",
    "href": "normas.html#ejemplos",
    "title": "5  Norma de vectores",
    "section": "7.2 Ejemplos",
    "text": "7.2 Ejemplos\nVeamos algunos ejemplos de cálculo de normas matriciales inducidas en matrices y vectores específicos.\nEjemplo 1 Dada la matriz \\[A = \\begin{bmatrix} 2 & -1 \\\\ -5 & 4 \\end{bmatrix}\\] y la norma vectorial \\(L_{1}\\), calculemos la norma matricial inducida por \\(L_{1}\\).\n\\[|\\|A\\||_{1} = \\max\\{7,5\\}=7\\]\nNorma infinito\n\\[|\\|A\\||_{\\infty} = \\max\\{3,9\\}=9\\]"
  },
  {
    "objectID": "normas.html#normas-de-frobenius",
    "href": "normas.html#normas-de-frobenius",
    "title": "5  Norma de vectores",
    "section": "7.3 Normas de Frobenius",
    "text": "7.3 Normas de Frobenius\nLa norma de Frobenius es una norma matricial no inducida. La norma de Frobenius de una matriz \\(A\\) se define como:\n\\[|\\|A\\||_{F} = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} |a_{ij}|^{2}}\\]\n\nimport numpy as np\n\n# Crear una matriz de ejemplo\nA = np.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9]])\n\n# Calcular la norma de Frobenius (norma F)\nnorm_frobenius = np.linalg.norm(A, ord='fro')\nprint(\"Norma de Frobenius de A:\", norm_frobenius)\n\n# Calcular la norma infinito (norma ∞)\nnorm_inf = np.linalg.norm(A, ord=np.inf)\nprint(\"Norma infinito de A:\", norm_inf)\n\n# Calcular la norma 1 (norma 1)\nnorm_1 = np.linalg.norm(A, ord=1)\nprint(\"Norma 1 de A:\", norm_1)\n\nNorma de Frobenius de A: 16.881943016134134\nNorma infinito de A: 24.0\nNorma 1 de A: 18.0"
  },
  {
    "objectID": "gramsmith.html",
    "href": "gramsmith.html",
    "title": "9  Factorización QR",
    "section": "",
    "text": "10 Nota\nSi \\(A_{n\\times n}\\), donde cada una de sus columnas son linealmente idependientes entonces las columnas de \\(Q\\) forman una base para $^n $. Además la matriz \\(Q\\) es ortonormal, es decir \\(QQ^T=Q^TQ=I\\).\nLa factorización \\(QR\\) tiene muchas aplicaciones, las más relevantes son: soluciones de ecuaciones y mínimos cuadrados. Para mostrar la primera aplicación, trabajaremos con la interpolación de un polinomio de grado 7.\nSea los puntos \\(x_0=2.0\\), \\(x_1=1.2\\),…,\\(x_{10}=4.0\\), puntos igualmente espaciados entre el intervalo \\([2,4]\\) y sea el conjunto \\(y_i=1+x_i+x_1^2+\\cdots+x_i^{10}\\) para \\(0\\leq i\\leq 10\\). Obviamente el polinomio que que une a estos puntos es \\(P(x)=1+x+x^2+\\cdots+x^{10}\\). supongamos que no conocemos el polinomio \\(P(x)\\), y lo queremos encontrar, de esta forma podemos decir que \\[P(x)=c_0+c_1x+\\cdots+c_{10}x^{10},\\] de esta forma podemos plantear el sistemas de ecuaciones\n\\[\\begin{pmatrix}\n1&x_0&x_0^2 &  & x^{10}_{0}\\\\\n\\vdots&\\vdots & \\vdots & & \\vdots\\\\\n1&x_{10}&x_{10}^2 &  & x^{10}_{10}\n\\end{pmatrix}\\begin{pmatrix}c_0\\\\\\vdots\\\\c_{10}\\end{pmatrix}=\\begin{pmatrix}y_0\\\\\\vdots\\\\y_{10}\\end{pmatrix},\\] Note que este sistema tiene una solución la cual es el vector \\(c\\) que contiene los coeficientes del polinomio \\(P(x)\\). para resolver este problema vamos a usar la factorización \\(QR\\), para ello escribimos de forma matricial el sistema de ecuaciones anterior\n\\[Ac=y,\\] susituyendo \\(A=QR\\) tenemos que \\[QRc=y,\\] multiplicando por \\(Q^T\\) a ambos lados de la ecuación anterior tenemos que \\[Q^TQRc=Q^Ty,\\] como \\(Q^TQ=I\\) entonces \\[Rc=Q^Ty,\\] finalmente como \\(R\\) es una matriz triangular superior, podemos resolver el sistema de ecuaciones anterior de forma sencilla."
  },
  {
    "objectID": "gramsmith.html#pregunta",
    "href": "gramsmith.html#pregunta",
    "title": "9  Factorización QR",
    "section": "10.1 Pregunta",
    "text": "10.1 Pregunta\n¿Qué podemos decir de las filas de la matriz de \\(Q\\) si \\(A_{n\\times m}\\) con \\(n>m\\)? ¿ De los productos \\(Q^tQ\\) y \\(QQ^T\\)?"
  },
  {
    "objectID": "kmean.html",
    "href": "kmean.html",
    "title": "6  K-mean",
    "section": "",
    "text": "K-mean es un algoritmo de aprendizaje no supervisado que agrupa los datos en K grupos distintos. El número de grupos K es un parámetro que se debe especificar por el usuario. El algoritmo funciona iterativamente para asignar cada dato a uno de los K grupos basado en las características que se proporcionan. Los datos se agrupan en función de la similitud de sus características.\nPara realizar esta labor, el algoritmo K-mean utiliza la distancia euclidiana para asignar cada dato a un grupo, con el objetivo de que la suma de las distancias al cuadrado dentro de cada grupo sea mínima."
  },
  {
    "objectID": "regresion.html",
    "href": "regresion.html",
    "title": "11  Regresión multivariada",
    "section": "",
    "text": "12 Ejemplo 3\nLinealización para encontrar una relación entra la altura y el peso usando la lew de potencias\n\\[y=c_1e^{c_2t}\\] peso versus altura\nEl modelo de la ecuación \\[y=c_1te^{c_2t}\\] se puede utilizar para ajustar los datos de la concentración de un medicamento en la sangre de un paciente.\nModelos La temperatura en Washington, D.C., desde enero 1 de 2001 es listada en la siguiente tabla.\nSuponemos que el modelo se comporta de la forma\n\\[y=c_1+c_2\\cos(2\\pi t)+c_3\\sin(2\\pi t)\\] use mínimos cuadrados para encontrar los parámetros \\(c_i\\)\ndatos disponibles"
  },
  {
    "objectID": "regresion.html#ejemplo",
    "href": "regresion.html#ejemplo",
    "title": "11  Regresión multivariada",
    "section": "11.1 Ejemplo",
    "text": "11.1 Ejemplo\nUn estudio quiere generar un modelo que permita predecir la esperanza de vida media de los habitantes de una ciudad en función de diferentes variables. Se dispone de información sobre la esperanza de vida media de los habitantes de 50 ciudades, junto con información sociodemográfica de cada una de ellas. En concreto, se conoce: el número de habitantes, nivel de analfabetismo, ingresos, asesinatos, cantidad de universitarios, heladas, área y densidad poblacional.\n\nimport pandas as pd\nurl = (\n    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/'\n    'Estadistica-machine-learning-python/master/data/state_x77.csv'\n)\ndatos = pd.read_csv(url, sep=',')\ndisplay(datos.info())\ndatos.head(3)\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 50 entries, 0 to 49\nData columns (total 9 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   habitantes      50 non-null     int64  \n 1   ingresos        50 non-null     int64  \n 2   analfabetismo   50 non-null     float64\n 3   esp_vida        50 non-null     float64\n 4   asesinatos      50 non-null     float64\n 5   universitarios  50 non-null     float64\n 6   heladas         50 non-null     int64  \n 7   area            50 non-null     int64  \n 8   densidad_pobl   50 non-null     float64\ndtypes: float64(5), int64(4)\nmemory usage: 3.6 KB\n\n\nNone\n\n\n\n\n\n\n  \n    \n      \n      habitantes\n      ingresos\n      analfabetismo\n      esp_vida\n      asesinatos\n      universitarios\n      heladas\n      area\n      densidad_pobl\n    \n  \n  \n    \n      0\n      3615\n      3624\n      2.1\n      69.05\n      15.1\n      41.3\n      20\n      50708\n      71.290526\n    \n    \n      1\n      365\n      6315\n      1.5\n      69.31\n      11.3\n      66.7\n      152\n      566432\n      0.644384\n    \n    \n      2\n      2212\n      4530\n      1.8\n      70.55\n      7.8\n      58.1\n      15\n      113417\n      19.503249\n    \n  \n\n\n\n\n\n\n\n\n\n\nEjercicios\n\n\n\n\n\nRespondan las siguientes preguntas:\n\n¿Podríamos encontrar una relación entre independencia lineal de dos variables y su correlación?\nHaga la regresión lineal con habitantes, asesinatos y densidad poblacional como variables explicativas. ¿Que puede decir de los valores de \\(\\beta\\)?\nEncuentre el valor del error cuadrático medio."
  },
  {
    "objectID": "regresion.html#linealización-de-modelos-no-lineales",
    "href": "regresion.html#linealización-de-modelos-no-lineales",
    "title": "11  Regresión multivariada",
    "section": "11.2 Linealización de modelos no lineales",
    "text": "11.2 Linealización de modelos no lineales\nEn ocasiones, los modelos no siguen una relación lineal entre las variables explicativas y la variable de respuesta. En estos casos, se puede linealizar el modelo, es decir, transformar el modelo no lineal en uno lineal. Por ejemplo, si se tiene un modelo de la forma: El crecimiento de una población de bacterias sigue una ley exponencial, es decir, el número de bacterias en el tiempo \\(t\\) es de la forma:\n\\[N(t) = N_0 e^{rt},\\]\ndonde \\(N_0\\) es el número inicial de bacterias, \\(r\\) es la tasa de crecimiento y \\(t\\) es el tiempo. Si tomamos logaritmos en ambos lados de la ecuación, obtenemos:\n\\[\\log(N(t)) = \\log(N_0) + rt,\\]\nNote de esta forma podemos lienalizar el modelo."
  },
  {
    "objectID": "regresion.html#ejemplo-2",
    "href": "regresion.html#ejemplo-2",
    "title": "11  Regresión multivariada",
    "section": "11.3 Ejemplo 2",
    "text": "11.3 Ejemplo 2\nLa ecuación que mas aproxima el comportamiento de la venta de carros es \\[y=c_1e^{c_2t}\\] Podemos lienalizar este modelo para que usando mínimos cuadrados encontremos los parametros \\(c_1\\) y \\(c_2\\) que mejor ajustan el modelo a los datos\n\nimport xml.etree.ElementTree as ET\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndf = pd.read_csv('datos/cpu.csv')\nprint(df.head())\nplt.plot(df['transistors'],'ro')\n\n    CPU  year  transistors\n0  4004  1971         2250\n1  8008  1972         2500\n2  8080  1974         5000\n3  8086  1978        29000\n4   286  1982       120000"
  },
  {
    "objectID": "minimosnonlinear.html#algortimo",
    "href": "minimosnonlinear.html#algortimo",
    "title": "12  Newton -Raphson Para sistemas",
    "section": "12.1 Algortimo",
    "text": "12.1 Algortimo\nEl algoritmo para el método de Newton para sistemas de ecuaciones no lineales es el siguiente: Imput : \\(F(x)\\), \\(x_0\\), \\(Tolerancia\\), \\(N_{max}\\) Output: \\(x\\) o un mensaje de error\n\n\\(k = 0\\)\nMientras \\(k < N_{max}\\) hacer:\n\nCalcular \\(F(x_k)\\)\nSi \\(||F(x_k)|| < Tolerancia\\) entonces\n\nRetornar \\(x_k\\)\n\nCalcular la matriz jacobiana \\(DF(x_k)\\)\n\nPreguntar si la matriz jacobiana es singular\n\nSi es singular entonces\n\nRetornar un mensaje de error\n\n\n\nResolver el sistema de ecuaciones lineales \\(D F(x_k)(x - x_k) = -F(x_k)\\)\n\\(x_{k+1} = x_k + x\\)\n\\(k = k + 1\\)\n\nRetornar un mensaje de error\n\nExisten otros métodos para resolver sistemas de ecuaciones no lineales, sin embargo el método de Newton es el más utilizado, aunque necesita el cálculo de la derivada. Un método alternativo para encontrar la solución de un sistema en varias variables es el método de Broyden. El método de Broyden es un método iterativo que no necesita el cálculo de la derivada. el cual se puede considerar como una evolución del método de la secante, y esta determinado por la siguiente fórmula:\n\nencontrar una buena aproximación de la matriz jacobiana \\(D_{n-1} F(x_0)\\)\nresolver el sistema de ecuaciones lineales \\(D_n=D_{n-1}+\\frac{\\Delta F_n-D_{n-1}\\Delta x_n}{||\\Delta x_n||^2}\\Delta x^T_n\\)\nContinuar según el método de Newton\n\ndonde \\(\\Delta F_n=F(x_n)-F(x_{n-1})\\) y \\(\\Delta x_n=x_n-x_{n-1}\\)\n\ndef broyden_method(F, x0, max_iterations, tolerance):\n    x = x0\n    J = np.eye(len(x0))  # Initial approximation of the Jacobian matrix\n    Fx = F(x)\n    \n    for _ in range(max_iterations):\n        delta_x = np.linalg.solve(J, -Fx)  # Solve the linear system J * delta_x = -F(x)\n        x_new = x + delta_x\n        Fx_new = F(x_new)\n        delta_F = Fx_new - Fx\n        \n        if np.linalg.norm(delta_F) < tolerance:\n            return x_new\n        \n        J += np.outer((delta_F - J @ delta_x), delta_x) / np.linalg.norm(delta_x)**2\n        x = x_new\n        Fx = Fx_new\n    \n    raise ValueError(\"Broyden method did not converge within the specified number of iterations.\")"
  },
  {
    "objectID": "minimosnonlinear.html#método-de-gradiente-descendente",
    "href": "minimosnonlinear.html#método-de-gradiente-descendente",
    "title": "12  Newton -Raphson Para sistemas",
    "section": "12.2 Método de Gradiente descendente",
    "text": "12.2 Método de Gradiente descendente\nEl método de gradiente descendiente es un método usado para encontrar máximos y mínimos de funciones. La idea es que si tenemos una función \\(f(x)\\), entonces el gradiente de \\(f(x)\\) nos da la dirección en la que la función crece más rápido. Si queremos encontrar el mínimo de la función, entonces debemos ir en la dirección opuesta al gradiente. De esta forma el método de gradiente descendente consiste en ir en la dirección opuesta al gradiente de la función. Si la función es convexa, entonces el método de gradiente descendente converge a un mínimo global. Si la función no es convexa, entonces el método de gradiente descendente converge a un mínimo local.\nPara poder adoptar este método vamos a explicar en que consiste el método de gradiente descendente. Supongamos que tenemos una función \\(f(x)\\), y queremos encontrar el mínimo de la función. Entonces el método de gradiente descendente consiste en iterar la siguiente fórmula:\n\\[x_{k+1} = x_k - \\alpha \\nabla f(x_k)\\]\ndonde \\(\\alpha\\) es un número positivo que se conoce como el tamaño de paso, machime learning se conoce como tasa de aprendizaje. Si \\(\\alpha\\) es muy grande, entonces el método de gradiente descendente puede diverger. Si \\(\\alpha\\) es muy pequeño, entonces el método de gradiente descendente puede converger muy lentamente. El método de gradiente descendente es un método iterativo, por lo que se puede detener en cualquier momento. Si la función es convexa, entonces el método de gradiente descendente converge a un mínimo global. Si la función no es convexa, entonces el método de gradiente descendente converge a un mínimo local.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Define the function to optimize\ndef f(x, y):\n    return x**2 + y**2\n\n# Define the gradient of the function\ndef grad_f(x, y):\n    return np.array([2*x, 2*y])\n\n# Define the gradient descent algorithm\ndef gradient_descent(start_x, start_y, learning_rate, num_iterations):\n    x = start_x\n    y = start_y\n    trajectory = [(x, y)]\n    \n    for _ in range(num_iterations):\n        gradient = grad_f(x, y)\n        x -= learning_rate * gradient[0]\n        y -= learning_rate * gradient[1]\n        trajectory.append((x, y))\n    \n    return trajectory\n\n# Set the initial point, learning rate, and number of iterations\nstart_x = 3\nstart_y = 3\nlearning_rate = 0.1\nnum_iterations = 20\n\n# Run the gradient descent algorithm\ntrajectory = gradient_descent(start_x, start_y, learning_rate, num_iterations)\n\n# Plot the function and the trajectory\nx = np.linspace(-5, 5, 100)\ny = np.linspace(-5, 5, 100)\nX, Y = np.meshgrid(x, y)\nZ = f(X, Y)\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('f(x, y)')\n\ntrajectory = np.array(trajectory)\nax.plot(trajectory[:, 0], trajectory[:, 1], f(trajectory[:, 0], trajectory[:, 1]), 'r--', marker='o')\n\nplt.show()\n\n\n\n\nDe esta forma tenemos siguiente algoritmo para el método de gradiente descendente:"
  },
  {
    "objectID": "minimosnonlinear.html#algoritmo",
    "href": "minimosnonlinear.html#algoritmo",
    "title": "12  Newton -Raphson Para sistemas",
    "section": "12.3 Algoritmo",
    "text": "12.3 Algoritmo\nImput : \\(f(x)\\), \\(x_0\\), \\(\\alpha\\), \\(N_{max}\\) Output: \\(x\\) o un mensaje de error\n\n\\(k = 0\\)\nMientras \\(k < N_{max}\\) hacer:\n\nCalcular \\(\\nabla f(x_k)\\)\n\\(x_{k+1} = x_k - \\alpha \\nabla f(x_k)\\)\n\\(k = k + 1\\)\n\nRetornar \\(x_k\\)"
  },
  {
    "objectID": "minimosnonlinear.html#método-de-newton-para-minimización",
    "href": "minimosnonlinear.html#método-de-newton-para-minimización",
    "title": "12  Newton -Raphson Para sistemas",
    "section": "12.4 Método de Newton para minimización",
    "text": "12.4 Método de Newton para minimización\nEl método de Newton es un método iterativo para encontrar los mínimos de una función. La idea es que si tenemos una función \\(f(x)\\), entonces el método de Newton consiste en aproximar la función por un polinomio de segundo grado y encontrar el mínimo de este polinomio. Vamos a explicar este método para el caso de una función de una variable, y luego vamos a generalizar el método para el caso de una función de varias variables.\nSea \\(f(x)\\) una función doblemente diferenciable entonces podemos aproximar esta función por medio un polinomio de Taylor de segundo grado: \\[f(x+t)\\approx f(x)+f'(x)t+\\frac{1}{2}f''(x)t^2\\] Si queremos encontrar el mínimo de esta función, entonces debemos encontrar el valor de \\(t\\) que minimiza el polinomio. Para encontrar este valor, derivamos el polinomio con respecto a \\(t\\) e igualamos a cero: \\[f'(x)+f''(x)t=0\\] de esta forma obtenemos que el valor de \\(t\\) que minimiza el polinomio es: \\[t=-\\frac{f'(x)}{f''(x)}\\] Este valor de \\(t\\) es el valor que minimiza el polinomio, y por lo tanto es una buena aproximación al mínimo de la función. De esta forma el método de Newton consiste en iterar la siguiente fórmula: \\[x_{k+1}=x_k-\\frac{f'(x_k)}{f''(x_k)}\\] si la función es convexa y la segunda derivada es positiva, entonces el método de Newton converge a un mínimo global.\nPodemos generalizar el método de Newton para el caso de una función de varias variables. Sea \\(f(x)\\in\\mathcal{R}^N\\to\\mathcal{R}\\) una función doblemente diferenciable, entonces podemos aproximar esta función por medio de un polinomio de Taylor de segundo grado:\n\\[f(x+t)\\approx f(x)+t^T\\nabla f(x)+\\frac{1}{2}t^T\\nabla^2 f(x)t\\]\nrecuerde que \\(\\nabla f(x)\\) es el gradiente de \\(f(x)\\) y \\(\\nabla^2 f(x)\\) es la matriz hessiana de \\(f(x)\\). Si queremos encontrar el mínimo de esta función, entonces debemos encontrar el valor de \\(t\\) que minimiza el polinomio. Para encontrar este valor, derivamos el polinomio con respecto a \\(t\\) e igualamos a cero: \\[\\nabla f(x)+\\nabla^2 f(x)t=0\\] solucionando el sistema obtenemos que el valor de \\(t\\) \\[t=-\\nabla^2 f(x)^{-1}\\nabla f(x)\\] así el método de Newton consiste en iterar la siguiente fórmula: \\[x_{k+1}=x_k-\\nabla^2 f(x_k)^{-1}\\nabla f(x_k)\\]"
  },
  {
    "objectID": "minimosnonlinear.html#gauss-newton-método",
    "href": "minimosnonlinear.html#gauss-newton-método",
    "title": "12  Mewton -Raphson Para sistemas",
    "section": "12.5 Gauss-Newton Método",
    "text": "12.5 Gauss-Newton Método\nEl método de Gauss-Newton es un método iterativo para resolver problemas de mínimos cuadrados no lineales. La idea es que si tenemos un sistema de \\(m\\) ecuaciones con \\(n\\) incógnitas:\n\\[  r_1(x_1, x_2, \\ldots, x_n) = 0 \\] \\[  r_2(x_1, x_2, \\ldots, x_n) = 0 \\] \\[  \\vdots \\] \\[  r_m(x_1, x_2, \\ldots, x_n) = 0. \\] Podemos definir el vector residuo como: \\[r(x)=[r_1,\\dots,r_m]\\] de esta forma podemos definir la función de la suma de los errores cuadráticos:\n\\[  E(x_1,...,x_n)=\\frac{1}{2}(r^1+\\cdots r_m^2)=\\frac{1}{2}r^Tr.\\] Note que esta función tiene un mínimo (¿Por que?) Para minimizar \\(E\\), nosotros buscamos que el gradiente \\(\\nabla E\\) sea igual a cero.\n\\[0=\\nabla E(x)=\\nabla \\Big(\\frac{1}{2}r^Tr\\Big)=r(x)Dr(x)\\]\nAquí empleamos la regla del producto para el gradiente\n\\[\\nabla (u^Tv)=v^TDu+u^TDv\\]\ndonde\n\\[Du(x_1,\\cdots,x_n)= \\begin{bmatrix}\n\\nabla u_1 \\\\\n\\vdots \\\\\n\\nabla u_n\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "minimosnonlinear.html#gauss-newton-método-para-mínimos-cuadrados-no-lineales",
    "href": "minimosnonlinear.html#gauss-newton-método-para-mínimos-cuadrados-no-lineales",
    "title": "12  Newton -Raphson Para sistemas",
    "section": "12.5 Gauss-Newton Método para mínimos cuadrados no lineales",
    "text": "12.5 Gauss-Newton Método para mínimos cuadrados no lineales\nEl método de Gauss-Newton es un método iterativo para resolver problemas de mínimos cuadrados no lineales. La idea es que si tenemos un sistema de \\(m\\) ecuaciones con \\(n\\) incógnitas:\n\\[  r_1(x_1, x_2, \\ldots, x_n) = 0 \\] \\[  r_2(x_1, x_2, \\ldots, x_n) = 0 \\] \\[  \\vdots \\] \\[  r_m(x_1, x_2, \\ldots, x_n) = 0. \\] Podemos definir el vector residuo como: \\[r(x)=[r_1,\\dots,r_m]\\] de esta forma podemos definir la función de la suma de los errores cuadráticos:\n\\[  E(x_1,...,x_n)=\\frac{1}{2}(r^1+\\cdots r_m^2)=\\frac{1}{2}r^Tr.\\] Note que \\(E\\in\\mathcal{R}^n\\to\\mathcal{R}\\) esta función tiene un mínimo ¿Por qué?.\nNote que la serie de Taylor de \\(E(x)\\) es: \\[E(x) = E(x_0) + \\nabla E(x_0)(x - x_0) + \\frac{1}{2}(x-x_0)^T\\nabla^2E(x_0)(x-x_0).\\] de la misma forma podemos calcular la serie de taylor de \\(r(x)\\):\n\\[r(x) = r(x_0) + Dr(x_0)(x - x_0).\\]\nNote que \\(Dr(x_0)\\) es la matriz jacobiana de \\(r(x)\\) evaluada en \\(x_0\\). Además note que \\(\\nabla E(x_0)=Dr(x_0)^Tr(x_0)\\) y\n\\[\\nabla^2E(x)=Dr(x)^TDr(x)+S(x),\\]\ndonde \\(S(x)=\\sum_{i=1}^m r_i(x)D^2r_i(x)\\). De esta forma podemos aproximar la función \\(E(x)\\) por medio de un polinomio de segundo grado:\n\\[E(x) = \\frac{1}{2}r(x_0)^Tr(x_0) + (Dr(x_0)^Tr(x_0))^T(x - x_0) + \\frac{1}{2}(x-x_0)^T(Dr(x_0)^TDr(x_0)+S(x))(x-x_0).\\]\nAplicando el método de Newton para minimizar \\(E(x)\\) obtenemos la siguiente fórmula iterativa:\n\\[x_{k+1}=x_k-(Dr(x_k)^TDr(x_k)+S(x_k))^{-1}Dr(x_k)^Tr(x_k)\\]\nEl cual converge localmente. Note que aui tenemos el problema de calcular \\(mn^2\\) derivadas, por tanto el costo computacionales realmente es alto.\nPara econtar el método de Gauss-Newton, podemos pensar que omitimos el término \\(S(x)\\), de esta forma obtenemos\n\\[E(x) = \\frac{1}{2}r(x_0)^Tr(x_0) + (Dr(x_0)^Tr(x_0))^T(x - x_0) + \\frac{1}{2}(x-x_0)^T(Dr(x_0)^TDr(x_0))(x-x_0).\\]\ny al aplicar el metodo de Newton obtenemos la siguiente fórmula iterativa:\n\\[x_{k+1}=x_k-(Dr(x_k)^TDr(x_k))^{-1}Dr(x_k)^Tr(x_k)\\]\nnote que esto lo podemos escribir como:\n\\[(Dr(x_k)^TDr(x_k))\\delta=Dr(x_k)^Tr(x_k),\\] con \\[x_{k+1}=x_k+\\delta.\\]\nNote que si la matriz \\(Dr(x_k)^TDr(x_k)\\) tiene rango completo el método de Gauss-Newton converge localmente y tiene una única solución. Ademas, si la matriz \\(Dr(x_k)^TDr(x_k)\\), además podríamos usar al factorización QR para resolver el sistema de ecuaciones lineales.\nasi el algoritmo es\nAlgoritmo\nMinimizar \\(E(x)=\\frac{1}{2}r(x)^Tr(x)\\)\nSea \\(x_0\\) una aproximación inicial,\n\nA=Dr(x_k)\nCalcular \\(A^TA\\delta=-A^Tr(x_k)\\)\nCalcular \\(x_{k+1}=x_k+\\delta\\)\n\n\n\n\n\n\n\nTeorema\n\n\n\nSean \\(u,v\\) dos funciones vectoriales tal que \\(u,v\\in\\mathcal{R}^n\\to\\mathcal{R}^m\\), entonces \\[\\nabla (u^Tv)=v^TDu+u^TDv\\] donde \\(Du\\) es la matriz jacobiana de \\(u\\) y \\(Dv\\) es la matriz jacobiana de \\(v\\)."
  },
  {
    "objectID": "minimosnonlinear.html#método-de-levenberg-marquardt",
    "href": "minimosnonlinear.html#método-de-levenberg-marquardt",
    "title": "12  Newton -Raphson Para sistemas",
    "section": "12.6 Método de Levenberg-Marquardt",
    "text": "12.6 Método de Levenberg-Marquardt\nEL método de Levenberg-Marquardt es una modificación del metodo de Gauss-Newton, el consiste en adicionar un parámetro para acelerar la convergencia.\nAlgoritmo\nMinimizar \\(E(x)=\\frac{1}{2}r(x)^Tr(x)\\)\nSea \\(x_0\\) una aproximación inicial, \\(\\lambda>0\\) un parámetro de regularización,\n\nA=Dr(x_k)\nCalcular \\(A^TA+\\lambda diag(A^TA)\\delta=-A^Tr(x_k)\\)\nCalcular \\(x_{k+1}=x_k+\\delta\\)\n\nnote que si \\(\\lambda=0\\) es el mismo caso de Gauss-Newton.\nEjercicio Aplicar el método de Gauus- Newton y Levenberg-Marquardt para resolver el siguiente problema de mínimos cuadrados no lineales: \\(y=c_1e^{-c_2x(t-c_3)^2}\\) para los puntos \\((t_i,y_i)=\\{(1,3),(2,5),(2,7),(3,5),(4,1)\\}\\)\nRta/ \\(c_1=6.301, c_2=-0.5088, c_3=2.249\\)\npara más detalles de esta sección ver Sun and Yuan (2006) y Sauer (2018).\n\n\n\n\nSauer, T. 2018. Numerical Analysis. Pearson Education. https://books.google.com.co/books?id=gAVTDwAAQBAJ.\n\n\nSun, Wenyu, and Ya-Xiang Yuan. 2006. Optimization Theory and Methods: Nonlinear Programming. Vol. 1. Springer Science & Business Media."
  }
]